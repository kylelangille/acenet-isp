{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e331b9e3-7151-41c4-bc0a-e9c3938ce432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140b729b-d3e8-418f-aff5-4d2f8cef7b26",
   "metadata": {},
   "source": [
    "### Clean and standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed9123ef-0216-43b9-83bb-e44af5b28051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kylea\\AppData\\Local\\Temp\\ipykernel_10300\\3359841184.py:60: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[column_name].str.contains(pattern, case=False, regex=True, na=False)]\n"
     ]
    }
   ],
   "source": [
    "file_path = \"ufo_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df_dropped_nas = df.dropna(subset=[\"shape\", \"duration\", \"occurred_date_time\"])\n",
    "\n",
    "\n",
    "# Cleaning function takes in dataframe, column, and patterns to filter out worthless and/or inconsistent values\n",
    "# Function also standardizes the date and filters out the time in \"occurred_date_time\" column\n",
    "def clean_dataframe(df, column_name, patterns):\n",
    "    for pattern in patterns:\n",
    "        df = df[~df[column_name].str.contains(pattern, regex=True, na=False)]\n",
    "\n",
    "    df[column_name] = pd.to_datetime(df[column_name], errors=\"coerce\", format=\"%m/%d/%y %H:%M\")\n",
    "\n",
    "    df = df.dropna(subset=[column_name])\n",
    "\n",
    "    df[column_name] = df[column_name].dt.strftime('%m/%d/%y')\n",
    "    \n",
    "    return df\n",
    "\n",
    "column_name = \"occurred_date_time\"\n",
    "patterns = [r'\\?', r'unknown', r'&', r'ongoing']\n",
    "\n",
    "df_cleaned_dates = clean_dataframe(df_dropped_nas, column_name, patterns)\n",
    "\n",
    "# Cleaning function to standardize time measurement in \"duration\" column to seconds\n",
    "def clean_duration(df, column_name, filter_values):\n",
    "    pattern = \"|\".join(filter_values)\n",
    "    df_cleaned = df[~df[column_name].str.contains(pattern, case=False, na=False)]\n",
    "\n",
    "    def convert_to_seconds(duration):\n",
    "    \n",
    "        duration = duration.lower()\n",
    "\n",
    "        match = re.match(r'(\\d+)\\s*(seconds|minutes|hours?)', duration)\n",
    "\n",
    "        if match:\n",
    "            value = int(match.group(1))\n",
    "            unit = match.group(2)\n",
    "\n",
    "            if 'second' in unit:\n",
    "                return value\n",
    "            elif 'minute' in unit:\n",
    "                return value * 60\n",
    "            elif 'hour' in unit:\n",
    "                return value * 3600\n",
    "        return None\n",
    "        \n",
    "    df_cleaned.loc[:, column_name] = df_cleaned[column_name].apply(convert_to_seconds)\n",
    "    df_cleaned = df_cleaned.dropna(subset=[column_name])\n",
    "    return df_cleaned\n",
    "\n",
    "filter_values = [\"months\", \"years\", \"constant\", \"few\", \"ongoing\"]\n",
    "\n",
    "df_cleaned_duration = clean_duration(df_cleaned_dates, \"duration\", filter_values)\n",
    "\n",
    "# Cleaning function to filter out non-US locations\n",
    "def clean_city(df, column_name, filter_values):\n",
    "    pattern = \"|\".join(filter_values)\n",
    "    df = df[~df[column_name].str.contains(pattern, case=False, regex=True, na=False)]\n",
    "\n",
    "    df = df.dropna(subset=[column_name])\n",
    "\n",
    "    return df\n",
    "\n",
    "city_filters = [\"(Canada)\", \"(Portugal)\", \"(Spain)\", \"(Germany)\", \"(UK)\", \"(England)\", \"(Brazil)\", \"(Ecuador)\", \n",
    "                \"(Estonia)\", \"(Puerto Rico)\", \"(South Africa)\", \"(France)\", \"(Philippines)\", \"(Nigeria)\", \n",
    "                \"(Holland)\", \"(Australia)\", \"(Ireland)\", \"(Colombia)\", \"(Scotland)\", \"(Cyprus)\", \"(South Korea)\",\n",
    "                \"(Norway)\", \"(Croatia)\", \"(Italy)\", \"(Singapore)\", \"(Chile)\", \"(Malta)\", \"(Greece)\", \"(Syria)\", \n",
    "                \"(Sweden)\", \"(Kyrgyzstan)\", \"(Myanmar)\", \"(Japan)\", \"(Mexico)\", \"(Argentina)\", \"(Egypt)\", \"(Poland)\",\n",
    "                \"(Turkey)\", \"(Iraq)\", \"(India)\", \"(Jamaica)\", \"(Malaysia)\", \"(Venezuela)\", \"(Israel)\", \"(Kosovo)\", \n",
    "                \"(Belize)\", \"(Belgium)\", \"(Jordan)\", \"(Costa Rica)\", \"(Netherlands)\", \"(The Netherlands)\", \"(New Zealand)\",\n",
    "                \"(Corsica)\", \"(in former Yugoslavia)\", \"(Bahamas)\", \"(location unspecified)\", \"(Serbia)\"] \n",
    "\n",
    "df_cleaned_city = clean_city(df_cleaned_duration, \"city\", city_filters)\n",
    "\n",
    "df_cleaned_city.dropna()\n",
    "df_cleaned_city['shape'] = df_cleaned_city['shape'].str.lower()\n",
    "df_cleaned_city['state'] = df_cleaned_city['state'].str.upper()\n",
    "\n",
    "output_path = \"cleaned_data.csv\"\n",
    "df_cleaned_city.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988a576c-bad1-4fe6-b578-b2128b3ceb1d",
   "metadata": {},
   "source": [
    "### Calculate basic statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfb3ce48-339d-446e-a971-951447229098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean duration: 940.0\n",
      "Median duration: 240.0\n",
      "Mode duration: 300.0\n",
      "\n",
      "\n",
      "state\n",
      "AB     346.800000\n",
      "AK    1161.898601\n",
      "AL    1001.841918\n",
      "AR     950.639640\n",
      "AZ    1181.462838\n",
      "BC     257.444444\n",
      "CA     846.606674\n",
      "CO     890.937357\n",
      "CT     937.229746\n",
      "DC     552.078125\n",
      "DE     686.480874\n",
      "FL     941.028173\n",
      "GA     948.928508\n",
      "HI     977.922819\n",
      "IA     765.671488\n",
      "ID     973.246296\n",
      "IL     749.204927\n",
      "IN     852.385051\n",
      "KS    1086.006369\n",
      "KY    1036.077266\n",
      "LA     771.297539\n",
      "MA     893.998214\n",
      "MB    8510.000000\n",
      "MD     851.558999\n",
      "ME    1110.417695\n",
      "MI     907.054832\n",
      "MN     983.835771\n",
      "MO     903.386835\n",
      "MS    1167.536184\n",
      "MT    1108.045977\n",
      "NB      40.000000\n",
      "NC     904.314650\n",
      "ND     861.902439\n",
      "NE    1084.672794\n",
      "NH     897.764000\n",
      "NJ     824.870355\n",
      "NM    1287.278891\n",
      "NS      10.000000\n",
      "NV    1255.871942\n",
      "NY     823.685821\n",
      "OH    1065.846323\n",
      "OK    1260.429293\n",
      "ON     480.000000\n",
      "OR     954.332398\n",
      "PA    1035.523047\n",
      "RI     706.211155\n",
      "SC     853.958292\n",
      "SD    1214.745223\n",
      "SK     960.000000\n",
      "TN    1190.494737\n",
      "TX     931.123488\n",
      "UT    1192.556856\n",
      "VA     882.561807\n",
      "VT     696.483395\n",
      "WA     886.372212\n",
      "WI     755.888303\n",
      "WV     864.295165\n",
      "WY    1124.205882\n",
      "YT     180.000000\n",
      "Name: duration, dtype: float64\n",
      "           id occurred_date_time            city state     shape  duration  \\\n",
      "4      147509           07/11/19   Bowling Green    KY     other    2700.0   \n",
      "11     147503           07/11/19       Glenville    WV   unknown    3600.0   \n",
      "12     147468           07/10/19          Duluth    GA  cylinder    7200.0   \n",
      "16     147501           07/10/19     Santa Maria    CA    sphere    1800.0   \n",
      "20     147464           07/10/19        Columbus    OH   unknown    3600.0   \n",
      "...       ...                ...             ...   ...       ...       ...   \n",
      "54345  109807           07/06/05        Portland    OR    sphere    3600.0   \n",
      "54346  143818           12/30/99     Saint Cloud    FL    circle    2400.0   \n",
      "54352  146112           04/11/01          Harlem    NY    circle    1500.0   \n",
      "54353   19571           09/16/01  Bonita Springs    FL    circle    7200.0   \n",
      "54354   13896           08/24/00           Adams    NY     light    3600.0   \n",
      "\n",
      "                                                 summary  \\\n",
      "4      Me and my girlfriend watched as we first seen ...   \n",
      "11     ((HOAX??))  Fast moving across the night sky, ...   \n",
      "12     Several bright lights.  Some where solid white...   \n",
      "16     Wednesday July, 6 around 7:30pm there was a la...   \n",
      "20     Hovering in one spot with red, green and white...   \n",
      "...                                                  ...   \n",
      "54345                Historical UFO sighting--July 1905.   \n",
      "54346  Bright light, I have a photograph and video.  ...   \n",
      "54352  Silent, shiny object moved up, down, across, t...   \n",
      "54353                         THEY  WERE IN A Z PATTERN.   \n",
      "54354                   8 ufos sighted over midnight sky   \n",
      "\n",
      "                                             details_url posted_date  \n",
      "4      http://www.nuforc.org/webreports/147/S147509.html     7/12/19  \n",
      "11     http://www.nuforc.org/webreports/147/S147503.html     7/12/19  \n",
      "12     http://www.nuforc.org/webreports/147/S147468.html     7/12/19  \n",
      "16     http://www.nuforc.org/webreports/147/S147501.html     7/12/19  \n",
      "20     http://www.nuforc.org/webreports/147/S147464.html     7/12/19  \n",
      "...                                                  ...         ...  \n",
      "54345  http://www.nuforc.org/webreports/109/S109807.html      6/4/14  \n",
      "54346  http://www.nuforc.org/webreports/143/S143818.html    11/24/18  \n",
      "54352  http://www.nuforc.org/webreports/146/S146112.html     5/14/19  \n",
      "54353   http://www.nuforc.org/webreports/019/S19571.html    10/12/01  \n",
      "54354   http://www.nuforc.org/webreports/013/S13896.html     9/17/00  \n",
      "\n",
      "[7716 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# import cleaned dataset\n",
    "cleaned_file_path = \"cleaned_data.csv\"\n",
    "df = pd.read_csv(cleaned_file_path)\n",
    "\n",
    "# Mean, median, and mode of duration\n",
    "mean_duration = df['duration'].mean().round(2)\n",
    "median_duration = df['duration'].median()\n",
    "mode_duration = df['duration'].mode()[0]\n",
    "\n",
    "print(f\"Mean duration: {mean_duration}\")\n",
    "print(f\"Median duration: {median_duration}\")\n",
    "print(f\"Mode duration: {mode_duration}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Mean duration for each state\n",
    "state_group = df.groupby('state')['duration'].mean()\n",
    "print(state_group)\n",
    "\n",
    "# Find outlier durations using interquartile range method\n",
    "\n",
    "# get q1 and q3\n",
    "q1 = df['duration'].quantile(0.25)\n",
    "q3 = df['duration'].quantile(0.75)\n",
    "\n",
    "# get iqr\n",
    "iqr = q3 - q1\n",
    "\n",
    "# get upper and lower bounds\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "# get outliers\n",
    "outliers = df[(df['duration'] < lower_bound) | (df['duration'] > upper_bound)]\n",
    "\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83f6d1e3-1362-4e69-b56f-525f126d80f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most commonly reported shape is light with 11577 reports\n",
      "\n",
      "\n",
      "shape\n",
      "light        11577\n",
      "circle        6195\n",
      "triangle      5193\n",
      "fireball      4597\n",
      "sphere        3878\n",
      "unknown       3718\n",
      "other         3585\n",
      "disk          3023\n",
      "oval          2473\n",
      "formation     1792\n",
      "changing      1362\n",
      "cigar         1311\n",
      "rectangle      947\n",
      "flash          946\n",
      "cylinder       877\n",
      "diamond        829\n",
      "chevron        723\n",
      "teardrop       477\n",
      "egg            469\n",
      "cone           207\n",
      "cross          172\n",
      "delta            3\n",
      "flare            1\n",
      "changed          1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Most commonly reported shape\n",
    "\n",
    "most_common_shape = df['shape'].value_counts().idxmax()\n",
    "most_common_shape_count = df['shape'].value_counts().max()\n",
    "\n",
    "print(f\"The most commonly reported shape is {most_common_shape} with {most_common_shape_count} reports\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# count of all reported shapes\n",
    "\n",
    "shape_counts = df['shape'].value_counts()\n",
    "\n",
    "print(f\"{shape_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b411ef9-6dc4-4b87-80d5-8a9a948b507c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CA has the most reported sightings with 6773 total sightings\n",
      "state\n",
      "CA    6773\n",
      "FL    3443\n",
      "WA    2735\n",
      "NY    2419\n",
      "TX    2397\n",
      "AZ    2072\n",
      "PA    2061\n",
      "OH    1822\n",
      "IL    1786\n",
      "NC    1570\n",
      "MI    1459\n",
      "OR    1426\n",
      "CO    1309\n",
      "NJ    1211\n",
      "VA    1173\n",
      "GA    1133\n",
      "MA    1120\n",
      "MO    1109\n",
      "SC    1007\n",
      "TN     950\n",
      "WI     949\n",
      "MN     889\n",
      "IN     883\n",
      "MD     839\n",
      "CT     827\n",
      "NV     695\n",
      "KY     673\n",
      "NM     649\n",
      "UT     598\n",
      "OK     594\n",
      "AL     563\n",
      "ID     540\n",
      "NH     500\n",
      "ME     486\n",
      "IA     484\n",
      "KS     471\n",
      "LA     447\n",
      "AR     444\n",
      "MT     435\n",
      "WV     393\n",
      "MS     304\n",
      "HI     298\n",
      "AK     286\n",
      "NE     272\n",
      "VT     271\n",
      "RI     251\n",
      "DE     183\n",
      "WY     170\n",
      "SD     157\n",
      "ND     123\n",
      "DC      64\n",
      "AB      10\n",
      "BC       9\n",
      "ON       8\n",
      "MB       3\n",
      "SK       2\n",
      "YT       1\n",
      "NS       1\n",
      "NB       1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# most reports per state\n",
    "\n",
    "most_common_state = df['state'].value_counts().idxmax()\n",
    "most_common_state_count = df['state'].value_counts().max()\n",
    "\n",
    "print(f\"{most_common_state} has the most reported sightings with {most_common_state_count} total sightings\")\n",
    "\n",
    "# sightings per state\n",
    "\n",
    "state_counts = df['state'].value_counts()\n",
    "\n",
    "print(f\"{state_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1cb88f-6c6f-459d-b8ad-236424d9c834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
