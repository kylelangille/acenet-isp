import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
import seaborn as sns





file_path = "ufo_data.csv"
df = pd.read_csv(file_path)

df_dropped_nas = df.dropna(subset=["shape", "duration", "occurred_date_time"])


# Cleaning function takes in dataframe, column, and patterns to filter out worthless and/or inconsistent values
# Function also standardizes the date and filters out the time in "occurred_date_time" column
def clean_dataframe(df, column_name, patterns):
    for pattern in patterns:
        df = df[~df[column_name].str.contains(pattern, regex=True, na=False)]

    df[column_name] = pd.to_datetime(df[column_name], errors="coerce", format="%m/%d/%y %H:%M")

    df = df.dropna(subset=[column_name])

    df[column_name] = df[column_name].dt.strftime('%m/%d/%y')
    
    return df

column_name = "occurred_date_time"
patterns = [r'\?', r'unknown', r'&', r'ongoing']

df_cleaned_dates = clean_dataframe(df_dropped_nas, column_name, patterns)

# Cleaning function to standardize time measurement in "duration" column to seconds
def clean_duration(df, column_name, filter_values):
    pattern = "|".join(filter_values)
    df_cleaned = df[~df[column_name].str.contains(pattern, case=False, na=False)]

    def convert_to_seconds(duration):
    
        duration = duration.lower()

        match = re.match(r'(\d+)\s*(seconds|minutes|hours?)', duration)

        if match:
            value = int(match.group(1))
            unit = match.group(2)

            if 'second' in unit:
                return value
            elif 'minute' in unit:
                return value * 60
            elif 'hour' in unit:
                return value * 3600
        return None
        
    df_cleaned.loc[:, column_name] = df_cleaned[column_name].apply(convert_to_seconds)
    df_cleaned = df_cleaned.dropna(subset=[column_name])
    return df_cleaned

filter_values = ["months", "years", "constant", "few", "ongoing"]

df_cleaned_duration = clean_duration(df_cleaned_dates, "duration", filter_values)

# Cleaning function to filter out non-US locations
def clean_city(df, column_name, filter_values):
    pattern = "|".join(filter_values)
    df = df[~df[column_name].str.contains(pattern, case=False, regex=True, na=False)]

    df = df.dropna(subset=[column_name])

    return df

city_filters = ["(Canada)", "(Portugal)", "(Spain)", "(Germany)", "(UK)", "(England)", "(Brazil)", "(Ecuador)", 
                "(Estonia)", "(Puerto Rico)", "(South Africa)", "(France)", "(Philippines)", "(Nigeria)", 
                "(Holland)", "(Australia)", "(Ireland)", "(Colombia)", "(Scotland)", "(Cyprus)", "(South Korea)",
                "(Norway)", "(Croatia)", "(Italy)", "(Singapore)", "(Chile)", "(Malta)", "(Greece)", "(Syria)", 
                "(Sweden)", "(Kyrgyzstan)", "(Myanmar)", "(Japan)", "(Mexico)", "(Argentina)", "(Egypt)", "(Poland)",
                "(Turkey)", "(Iraq)", "(India)", "(Jamaica)", "(Malaysia)", "(Venezuela)", "(Israel)", "(Kosovo)", 
                "(Belize)", "(Belgium)", "(Jordan)", "(Costa Rica)", "(Netherlands)", "(The Netherlands)", "(New Zealand)",
                "(Corsica)", "(in former Yugoslavia)", "(Bahamas)", "(location unspecified)", "(Serbia)", "(Iran)", "(Pakistan)", 
                "(Finland)", "(Botswana)", "(Dominican Republic)", r"\?", "(Saudi Arabia)", "(Indonesia)", "(Denmark)", "(Cambodia)",
                "(Russia)", "(China)"] 

df_cleaned_city = clean_city(df_cleaned_duration, "city", city_filters)

# one last NaN drop
df_cleaned_city.dropna()

# standardize capitalization
df_cleaned_city['shape'] = df_cleaned_city['shape'].str.lower()
df_cleaned_city['state'] = df_cleaned_city['state'].str.upper()

output_path = "cleaned_data.csv"
df_cleaned_city.to_csv(output_path, index=False)





# import cleaned dataset
cleaned_file_path = "cleaned_data.csv"
df = pd.read_csv(cleaned_file_path)

# Mean, median, and mode of duration
mean_duration = df['duration'].mean().round(2)
median_duration = df['duration'].median()
mode_duration = df['duration'].mode()[0]

print(f"Mean duration: {mean_duration}")
print(f"Median duration: {median_duration}")
print(f"Mode duration: {mode_duration}")
print("\n")

# Mean duration for each state
state_group = df.groupby('state')['duration'].mean()
print(state_group)

# Find outlier durations using interquartile range method

# get q1 and q3
q1 = df['duration'].quantile(0.25)
q3 = df['duration'].quantile(0.75)

# get iqr
iqr = q3 - q1

# get upper and lower bounds
lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr

# get outliers
outliers = df[(df['duration'] < lower_bound) | (df['duration'] > upper_bound)]
print(outliers.describe())


# Most commonly reported shape
most_common_shape = df['shape'].value_counts().idxmax()
most_common_shape_count = df['shape'].value_counts().max()

print(f"The most commonly reported shape is {most_common_shape} with {most_common_shape_count} reports")
print("\n")

# count of all reported shapes
shape_counts = df['shape'].value_counts()
print(f"{shape_counts}")


# most reports per state

most_common_state = df['state'].value_counts().idxmax()
most_common_state_count = df['state'].value_counts().max()

print(f"{most_common_state} has the most reported sightings with {most_common_state_count} total sightings")

# sightings per state

state_counts = df['state'].value_counts()

print(f"{state_counts}")





# Histogram of reported duration

plt.figure(figsize=(10, 6))
sns.histplot(df['duration'], bins=30, kde=True)
plt.title('Distribution of Reported Durations')
plt.xlabel('Duration')
plt.ylabel('Frequency')
plt.savefig('plots/durationHistogram.png')
plt.show()

# Histogram is very skewed, but how much exactly?
skewness = df['duration'].skew()
print("skewness", skewness)
# 36. HIGHLY skewed duration data!!!!

# Bar plot of reported shapes

plt.figure(figsize=(12, 8))
sns.countplot(data=df, x='shape', order=df['shape'].value_counts().index)
plt.title("Number of Reported Shapes")
plt.xlabel('Shape')
plt.ylabel('Number of Reports')
plt.xticks(rotation=45)
plt.savefig('plots/shapeBarPlot.png')
plt.show()



